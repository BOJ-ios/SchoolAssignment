{"cells":[{"cell_type":"markdown","metadata":{"id":"QklLFdrtjr5a"},"source":["### 패키지 선언"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"7lV8TMctTuX7"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.datasets as dataset\n","import torchvision.transforms as transform\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"wglW_lW5j8RM"},"source":["## Dataset 다운로드"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"24F1ZKZ9j2y-"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 170498071/170498071 [00:42<00:00, 4037543.61it/s] \n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./cifar-10-python.tar.gz to ./\n","Files already downloaded and verified\n"]}],"source":["# Training dataset 다운로드\n","cifar10_train = dataset.CIFAR10(root = \"./\",\n","                            train = True,\n","                            transform = transform.ToTensor(),\n","                            download = True)\n","# Testing dataset 다운로드\n","cifar10_test = dataset.CIFAR10(root = \"./\",\n","                            train = False,\n","                            transform = transform.ToTensor(),\n","                            download = True)\n","\n","dataLabel = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]"]},{"cell_type":"markdown","metadata":{"id":"I65fUdO7uzp-"},"source":["## 신경망 모델 정의\n","- Fully Connected Layer -> nn.Linear(in_features, out_features)\n","- Convolutional Layer -> nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n","- ReLU -> nn.ReLU()\n","- Max Pooling -> nn.MaxPool2d(kernel_size, stride)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"oc4HgDKRtDFk"},"outputs":[],"source":["class SimplifiedVGG_Skip(nn.Module):\n","\n","  def __init__(self):\n","    super(SimplifiedVGG_Skip, self).__init__()\n","\n","    # 신경망 파라미터 초기화 (Conv 6개, FC 3개, ReLU, MaxPool)\n","    self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)    # Convolution: [3x3x3]x16, s1, p1\n","    self.conv1_2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)   # Convolution: [3x3x16]x32, s1, p1\n","\n","    self.conv2_1 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)   # Convolution: [3x3x32]x32, s1, p1\n","    self.conv2_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)   # Convolution: [3x3x32]x64, s1, p1\n","\n","    self.conv3_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)  # Convolution: [3x3x64]x128, s1, p1\n","    self.conv3_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1) # Convolution: [3x3x128]x256, s1, p1\n","\n","    self.fc1 = nn.Linear(in_features=4096, out_features=512)\n","    self.fc2 = nn.Linear(in_features=512, out_features=256)\n","    self.fc3 = nn.Linear(in_features=256, out_features=10)\n","\n","    self.relu = nn.ReLU()\n","    self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    # Skip connection* 위한 convolution layer 추가 선언\n","    self.conv_skip1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n","    self.conv_skip2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n","    self.conv_skip3 = nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, stride=1, padding=1)\n","  def forward(self, x):\n","    #-----------------------------------------------------------------\n","    # Convolution, ReLU, MaxPool layer\n","    skip_y = self.conv_skip1(x)\n","    y = self.relu(self.conv1_1(x))\n","    y = self.relu(self.conv1_2(y))\n","    y = y + skip_y\n","    y = self.max_pool(y)\n","\n","    skip_y = self.conv_skip2(y)\n","    y = self.relu(self.conv2_1(y))\n","    y = self.relu(self.conv2_2(y))\n","    y = y + skip_y\n","    y = self.max_pool(y)\n","\n","    \n","    skip_y = self.conv_skip3(y)\n","    y = self.relu(self.conv3_1(y))\n","    y = self.relu(self.conv3_2(y))\n","    y = y + skip_y\n","    y = self.max_pool(y)\n","    #-----------------------------------------------------------------\n","\n","    y = y.view(-1, 4096) # feature map 평탄화\n","\n","    #-----------------------------------------------------------------\n","    # Fully Connected, ReLU layer\n","    y = self.relu(self.fc1(y))\n","    y = self.relu(self.fc2(y))\n","    y = self.fc3(y)\n","    #-----------------------------------------------------------------\n","\n","    return y\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class SimplifiedVGG_Dense(nn.Module):\n","\n","  def __init__(self):\n","    super(SimplifiedVGG_Dense, self).__init__()\n","\n","    # 신경망 파라미터 초기화 (Conv 6개, FC 3개, ReLU, MaxPool)\n","    self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)    # Convolution: [3x3x3]x16, s1, p1\n","    self.conv1_2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)   # Convolution: [3x3x16]x32, s1, p1\n","\n","    self.conv2_1 = nn.Conv2d(in_channels=35, out_channels=32, kernel_size=3, stride=1, padding=1)   # Convolution: [3x3x32]x32, s1, p1\n","    self.conv2_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)   # Convolution: [3x3x32]x64, s1, p1\n","\n","    self.conv3_1 = nn.Conv2d(in_channels=99, out_channels=128, kernel_size=3, stride=1, padding=1)  # Convolution: [3x3x64]x128, s1, p1\n","    self.conv3_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1) # Convolution: [3x3x128]x256, s1, p1\n","\n","    self.fc1 = nn.Linear(in_features=4096, out_features=512)\n","    self.fc2 = nn.Linear(in_features=512, out_features=256)\n","    self.fc3 = nn.Linear(in_features=256, out_features=10)\n","\n","    self.relu = nn.ReLU()\n","    self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    \n","  def forward(self, x):\n","    #-----------------------------------------------------------------\n","    # Convolution, ReLU, MaxPool layer\n","    dense_y = x\n","    y = self.relu(self.conv1_1(x))\n","    y = self.relu(self.conv1_2(y))\n","    y = torch.cat([y, dense_y], dim = 1)\n","    y = self.max_pool(y)\n","\n","    dense_y = y\n","    y = self.relu(self.conv2_1(y))\n","    y = self.relu(self.conv2_2(y))\n","    y = torch.cat([y, dense_y], dim = 1)\n","    y = self.max_pool(y)\n","\n","    \n","    y = self.relu(self.conv3_1(y))\n","    y = self.relu(self.conv3_2(y))\n","    y = self.max_pool(y)\n","    #-----------------------------------------------------------------\n","\n","    y = y.view(-1, 4096) # feature map 평탄화\n","\n","    #-----------------------------------------------------------------\n","    # Fully Connected, ReLU layer\n","    y = self.relu(self.fc1(y))\n","    y = self.relu(self.fc2(y))\n","    y = self.fc3(y)\n","    #-----------------------------------------------------------------\n","\n","    return y\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ChannelAttention(nn.Module):\n","  def __init__(self, channels, reduction):\n","    super(ChannelAttention, self).__init__()\n","    self.gap = nn.AdaptiveAvgPool2d((1, 1))\n","    self.conv1 = nn.Conv2d(in_channels=channels, out_channels=channels//reduction, kernel_size=1)\n","    self.conv2 = nn.Conv2d(in_channels=channels//reduction, out_channels=channels, kernel_size=1)\n","    self.relu = nn.ReLU()\n","    self.sigmoid = nn.Sigmoid()\n","  def forward(self, x):\n","    ca_out = self.gap(x)\n","    ca_out = self.relu(self.conv1(ca_out))\n","    ca_out = self.sigmoid(self.conv2(ca_out))\n","    ca_out = ca_out.expand_as(x)\n","    y = x * ca_out\n","    return y"]},{"cell_type":"markdown","metadata":{"id":"5Fz63UdivjY3"},"source":["## Hyper-parameters 지정"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"DIdnVvA4vjCe"},"outputs":[],"source":["batch_size = 100\n","learning_rate = 0.1\n","training_epochs = 15\n","loss_function = nn.CrossEntropyLoss()\n","# network = SimplifiedVGG_Skip().to('cuda')\n","# network = SimplifiedVGG_Dense().to('cuda')\n","network = SimplifiedVGG_Skip()\n","optimizer = torch.optim.SGD(network.parameters(), lr = learning_rate)\n","\n","data_loader = DataLoader(dataset=cifar10_train,\n","                         batch_size=batch_size,\n","                         shuffle=True,\n","                         drop_last=True)"]},{"cell_type":"markdown","metadata":{"id":"dBPBa7th2oNx"},"source":["## CNN 학습을 위한 반복문 선언"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"_vKcxUMlvUJE"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1 Loss = 1.940684\n","Epoch: 2 Loss = 1.402239\n","Epoch: 3 Loss = 1.136096\n","Epoch: 4 Loss = 0.924144\n","Epoch: 5 Loss = 0.741712\n","Epoch: 6 Loss = 0.604022\n","Epoch: 7 Loss = 0.458180\n","Epoch: 8 Loss = 0.331679\n","Epoch: 9 Loss = 0.229350\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\LoewllZoe\\Documents\\GitHub\\SchoolAssignment\\머신러닝\\231116_Basecode_Simplified_VGG_CIFAR10\\231116_Basecode_Simplified_VGG_CIFAR10.ipynb 셀 12\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LoewllZoe/Documents/GitHub/SchoolAssignment/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/231116_Basecode_Simplified_VGG_CIFAR10/231116_Basecode_Simplified_VGG_CIFAR10.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m total_batch \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data_loader)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LoewllZoe/Documents/GitHub/SchoolAssignment/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/231116_Basecode_Simplified_VGG_CIFAR10/231116_Basecode_Simplified_VGG_CIFAR10.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m img, label \u001b[39min\u001b[39;00m data_loader:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LoewllZoe/Documents/GitHub/SchoolAssignment/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/231116_Basecode_Simplified_VGG_CIFAR10/231116_Basecode_Simplified_VGG_CIFAR10.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m   \u001b[39m# img = img.to('cuda')\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LoewllZoe/Documents/GitHub/SchoolAssignment/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/231116_Basecode_Simplified_VGG_CIFAR10/231116_Basecode_Simplified_VGG_CIFAR10.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m   \u001b[39m# label = label.to('cuda')\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/LoewllZoe/Documents/GitHub/SchoolAssignment/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/231116_Basecode_Simplified_VGG_CIFAR10/231116_Basecode_Simplified_VGG_CIFAR10.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m   pred \u001b[39m=\u001b[39m network(img)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LoewllZoe/Documents/GitHub/SchoolAssignment/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/231116_Basecode_Simplified_VGG_CIFAR10/231116_Basecode_Simplified_VGG_CIFAR10.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m   loss \u001b[39m=\u001b[39m loss_function(pred, label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LoewllZoe/Documents/GitHub/SchoolAssignment/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/231116_Basecode_Simplified_VGG_CIFAR10/231116_Basecode_Simplified_VGG_CIFAR10.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m   optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m# gradient 초기화\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\LoewllZoe\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\u001b[1;32mc:\\Users\\LoewllZoe\\Documents\\GitHub\\SchoolAssignment\\머신러닝\\231116_Basecode_Simplified_VGG_CIFAR10\\231116_Basecode_Simplified_VGG_CIFAR10.ipynb 셀 12\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LoewllZoe/Documents/GitHub/SchoolAssignment/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/231116_Basecode_Simplified_VGG_CIFAR10/231116_Basecode_Simplified_VGG_CIFAR10.ipynb#X12sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m y \u001b[39m=\u001b[39m y \u001b[39m+\u001b[39m skip_y\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LoewllZoe/Documents/GitHub/SchoolAssignment/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/231116_Basecode_Simplified_VGG_CIFAR10/231116_Basecode_Simplified_VGG_CIFAR10.ipynb#X12sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_pool(y)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/LoewllZoe/Documents/GitHub/SchoolAssignment/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/231116_Basecode_Simplified_VGG_CIFAR10/231116_Basecode_Simplified_VGG_CIFAR10.ipynb#X12sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m skip_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_skip3(y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LoewllZoe/Documents/GitHub/SchoolAssignment/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/231116_Basecode_Simplified_VGG_CIFAR10/231116_Basecode_Simplified_VGG_CIFAR10.ipynb#X12sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3_1(y))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LoewllZoe/Documents/GitHub/SchoolAssignment/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/231116_Basecode_Simplified_VGG_CIFAR10/231116_Basecode_Simplified_VGG_CIFAR10.ipynb#X12sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3_2(y))\n","File \u001b[1;32mc:\\Users\\LoewllZoe\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\LoewllZoe\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","File \u001b[1;32mc:\\Users\\LoewllZoe\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for epoch in range(training_epochs):\n","  network.train()\n","  avg_cost = 0\n","  total_batch = len(data_loader)\n","\n","  for img, label in data_loader:\n","    # img = img.to('cuda')\n","    # label = label.to('cuda')\n","    pred = network(img)\n","\n","    loss = loss_function(pred, label)\n","    optimizer.zero_grad() # gradient 초기화\n","    loss.backward()\n","    optimizer.step()\n","\n","    avg_cost += loss / total_batch\n","\n","  print('Epoch: %d Loss = %f'%(epoch+1, avg_cost))\n","\n","print('Learning finished')"]},{"cell_type":"markdown","metadata":{"id":"M-ewxZmlAPZ1"},"source":["## 학습이 완료된 모델을 이용해 정답률 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ae3-hzD67GG5"},"outputs":[],"source":["network.eval()\n","network = network.to('cpu')\n","img_test = torch.tensor(np.transpose(cifar10_test.data,(0,3,1,2))) / 255.\n","label_test = torch.tensor(cifar10_test.targets)\n","\n","with torch.no_grad(): # test에서는 기울기 계산 제외\n","  prediction = network(img_test) # 전체 test data를 한번에 계산\n","\n","  correct_prediction = torch.argmax(prediction, 1) == label_test\n","  accuracy = correct_prediction.float().mean()\n","  print('Accuracy:', accuracy.item())"]},{"cell_type":"markdown","metadata":{"id":"Ls_eHjdb-yjV"},"source":["## 예측 결과 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ck9OBIcpo-Tw"},"outputs":[],"source":["dataIdx = 2020\n","tmp_img = img_test[dataIdx]\n","tmp_label = label_test[dataIdx]\n","\n","with torch.no_grad():\n","  prediction = network(tmp_img)\n","\n","prediction_num = torch.argmax(prediction, 1)\n","\n","print(prediction)\n","print(\"prediction: \", dataLabel[prediction_num])\n","print(\"label: \", dataLabel[tmp_label])\n","\n","plt.imshow(np.transpose(tmp_img,(1,2,0)))\n","plt.show"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PA-a4ou_kABo"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1UiV9LAqa5SX190TsRvE1OfNPCJbc03zp","timestamp":1667979764680}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
